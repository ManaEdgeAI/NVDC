[2024-08-06 00:39:55.447] [info] llama_core in llama-core/src/lib.rs:433: Initializing the core context
[2024-08-06 00:40:20.684] [info] llama_core in llama-core/src/lib.rs:489: running mode: chat
[2024-08-06 00:40:20.684] [info] llama_core in llama-core/src/lib.rs:501: The core context has been initialized
[2024-08-06 00:40:20.685] [info] llama_core in llama-core/src/lib.rs:590: Getting the plugin info
[2024-08-06 00:40:20.685] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 00:40:20.685] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 00:40:20.685] [info] llama_core in llama-core/src/lib.rs:670: Getting the plugin info by the graph named default
[2024-08-06 00:40:20.685] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:40:20.686] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 95
[2024-08-06 00:40:20.686] [info] llama_core in llama-core/src/lib.rs:730: Plugin info: b2963(commit 95fb0aef)
[2024-08-06 00:40:37.414] [info] llama_core::models in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/models.rs:9: List models
[2024-08-06 00:40:43.662] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:48: tool choice: Some(None)
[2024-08-06 00:40:43.662] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:49: tools: None
[2024-08-06 00:40:43.662] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:50: stream mode: None
[2024-08-06 00:40:43.663] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 00:40:43.663] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 00:40:47.121] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 00:40:47.121] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:40:47.126] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 96
[2024-08-06 00:40:47.127] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 29, completion tokens: 0
[2024-08-06 00:41:33.051] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:41:33.052] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 521
[2024-08-06 00:41:33.058] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 00:41:33.058] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:41:33.059] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 98
[2024-08-06 00:41:33.059] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 29, completion tokens: 373
[2024-08-06 00:41:55.312] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:48: tool choice: Some(None)
[2024-08-06 00:41:55.312] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:49: tools: None
[2024-08-06 00:41:55.312] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:50: stream mode: None
[2024-08-06 00:41:55.313] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 00:41:55.313] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 00:41:56.276] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 00:41:56.276] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:41:56.276] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 98
[2024-08-06 00:41:56.277] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 18, completion tokens: 373
[2024-08-06 00:42:02.093] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:42:02.093] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 189
[2024-08-06 00:42:02.094] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 00:42:02.095] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:42:02.095] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 97
[2024-08-06 00:42:02.095] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 18, completion tokens: 42
[2024-08-06 00:42:21.165] [info] llama_core in llama-core/src/lib.rs:433: Initializing the core context
[2024-08-06 00:42:52.032] [info] llama_core in llama-core/src/lib.rs:489: running mode: chat
[2024-08-06 00:42:52.032] [info] llama_core in llama-core/src/lib.rs:501: The core context has been initialized
[2024-08-06 00:42:52.032] [info] llama_core in llama-core/src/lib.rs:590: Getting the plugin info
[2024-08-06 00:42:52.033] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 00:42:52.033] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 00:42:52.033] [info] llama_core in llama-core/src/lib.rs:670: Getting the plugin info by the graph named default
[2024-08-06 00:42:52.033] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:42:52.033] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 95
[2024-08-06 00:42:52.034] [info] llama_core in llama-core/src/lib.rs:730: Plugin info: b2963(commit 95fb0aef)
[2024-08-06 00:43:11.320] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:48: tool choice: Some(None)
[2024-08-06 00:43:11.320] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:49: tools: None
[2024-08-06 00:43:11.320] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:50: stream mode: None
[2024-08-06 00:43:11.322] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 00:43:11.322] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 00:43:12.769] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 00:43:12.769] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:43:12.770] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 96
[2024-08-06 00:43:12.771] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 19, completion tokens: 0
[2024-08-06 00:43:34.407] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:43:34.407] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 189
[2024-08-06 00:43:34.409] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 00:43:34.409] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:43:34.410] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 97
[2024-08-06 00:43:34.410] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 19, completion tokens: 41
[2024-08-06 00:43:39.746] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:48: tool choice: Some(None)
[2024-08-06 00:43:39.746] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:49: tools: None
[2024-08-06 00:43:39.746] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:50: stream mode: None
[2024-08-06 00:43:39.746] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 00:43:39.747] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 00:43:40.405] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 00:43:40.405] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:43:40.405] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 97
[2024-08-06 00:43:40.406] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 19, completion tokens: 41
[2024-08-06 00:43:43.584] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:43:43.585] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 112
[2024-08-06 00:43:43.724] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 00:43:43.724] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:43:43.724] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 97
[2024-08-06 00:43:43.725] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 19, completion tokens: 23
[2024-08-06 00:43:53.032] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:48: tool choice: Some(None)
[2024-08-06 00:43:53.032] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:49: tools: None
[2024-08-06 00:43:53.032] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:50: stream mode: None
[2024-08-06 00:43:53.033] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 00:43:53.033] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 00:43:53.677] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 00:43:53.677] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:43:53.677] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 97
[2024-08-06 00:43:53.678] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 30, completion tokens: 23
[2024-08-06 00:44:51.081] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:44:51.081] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 1200
[2024-08-06 00:44:51.084] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 00:44:51.084] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 00:44:51.084] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 98
[2024-08-06 00:44:51.085] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 30, completion tokens: 788
[2024-08-06 01:15:21.484] [info] llama_core in llama-core/src/lib.rs:433: Initializing the core context
[2024-08-06 01:15:41.366] [info] llama_core in llama-core/src/lib.rs:489: running mode: chat
[2024-08-06 01:15:41.366] [info] llama_core in llama-core/src/lib.rs:501: The core context has been initialized
[2024-08-06 01:15:41.366] [info] llama_core in llama-core/src/lib.rs:590: Getting the plugin info
[2024-08-06 01:15:41.366] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 01:15:41.366] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 01:15:41.366] [info] llama_core in llama-core/src/lib.rs:670: Getting the plugin info by the graph named default
[2024-08-06 01:15:41.367] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:15:41.367] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 95
[2024-08-06 01:15:41.367] [info] llama_core in llama-core/src/lib.rs:730: Plugin info: b2963(commit 95fb0aef)
[2024-08-06 01:15:57.966] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:48: tool choice: Some(None)
[2024-08-06 01:15:57.967] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:49: tools: None
[2024-08-06 01:15:57.967] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:50: stream mode: None
[2024-08-06 01:15:57.968] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 01:15:57.968] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 01:15:59.721] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 01:15:59.721] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:15:59.722] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 96
[2024-08-06 01:15:59.722] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 29, completion tokens: 0
[2024-08-06 01:16:43.174] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:16:43.174] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 672
[2024-08-06 01:16:43.177] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 01:16:43.177] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:16:43.177] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 98
[2024-08-06 01:16:43.178] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 29, completion tokens: 364
[2024-08-06 01:17:14.783] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:48: tool choice: Some(None)
[2024-08-06 01:17:14.783] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:49: tools: None
[2024-08-06 01:17:14.783] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:50: stream mode: None
[2024-08-06 01:17:14.784] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 01:17:14.784] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 01:17:15.400] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 01:17:15.400] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:17:15.400] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 98
[2024-08-06 01:17:15.401] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 26, completion tokens: 364
[2024-08-06 01:17:26.727] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:17:26.727] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 582
[2024-08-06 01:17:26.729] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 01:17:26.729] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:17:26.729] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 98
[2024-08-06 01:17:26.730] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 26, completion tokens: 138
[2024-08-06 01:18:59.302] [info] llama_core in llama-core/src/lib.rs:433: Initializing the core context
[2024-08-06 01:19:29.065] [info] llama_core in llama-core/src/lib.rs:489: running mode: chat
[2024-08-06 01:19:29.065] [info] llama_core in llama-core/src/lib.rs:501: The core context has been initialized
[2024-08-06 01:19:29.065] [info] llama_core in llama-core/src/lib.rs:590: Getting the plugin info
[2024-08-06 01:19:29.065] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 01:19:29.065] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 01:19:29.065] [info] llama_core in llama-core/src/lib.rs:670: Getting the plugin info by the graph named default
[2024-08-06 01:19:29.066] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:19:29.066] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 95
[2024-08-06 01:19:29.066] [info] llama_core in llama-core/src/lib.rs:730: Plugin info: b2963(commit 95fb0aef)
[2024-08-06 01:22:53.886] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:48: tool choice: Some(None)
[2024-08-06 01:22:53.886] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:49: tools: None
[2024-08-06 01:22:53.886] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:50: stream mode: None
[2024-08-06 01:22:53.887] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 01:22:53.887] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 01:22:55.768] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 01:22:55.769] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:22:55.769] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 96
[2024-08-06 01:22:55.770] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 25, completion tokens: 0
[2024-08-06 01:23:26.540] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:23:26.540] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 869
[2024-08-06 01:23:26.542] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 01:23:26.542] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:23:26.543] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 98
[2024-08-06 01:23:26.543] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 25, completion tokens: 195
[2024-08-06 01:28:33.323] [info] llama_core in llama-core/src/lib.rs:433: Initializing the core context
[2024-08-06 01:28:56.413] [info] llama_core in llama-core/src/lib.rs:489: running mode: chat
[2024-08-06 01:28:56.413] [info] llama_core in llama-core/src/lib.rs:501: The core context has been initialized
[2024-08-06 01:28:56.413] [info] llama_core in llama-core/src/lib.rs:590: Getting the plugin info
[2024-08-06 01:28:56.413] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 01:28:56.413] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 01:28:56.413] [info] llama_core in llama-core/src/lib.rs:670: Getting the plugin info by the graph named default
[2024-08-06 01:28:56.414] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:28:56.414] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 95
[2024-08-06 01:28:56.415] [info] llama_core in llama-core/src/lib.rs:730: Plugin info: b2963(commit 95fb0aef)
[2024-08-06 01:29:12.783] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:48: tool choice: Some(None)
[2024-08-06 01:29:12.783] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:49: tools: None
[2024-08-06 01:29:12.783] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:50: stream mode: None
[2024-08-06 01:29:12.784] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 01:29:12.784] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 01:29:14.446] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 01:29:14.447] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:29:14.447] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 96
[2024-08-06 01:29:14.448] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 25, completion tokens: 0
[2024-08-06 01:29:39.554] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:29:39.554] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 388
[2024-08-06 01:29:39.556] [info] llama_core::utils in llama-core/src/utils.rs:232: Get token info from the model named default.
[2024-08-06 01:29:39.556] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:29:39.556] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 97
[2024-08-06 01:29:39.557] [info] llama_core::utils in llama-core/src/utils.rs:271: prompt tokens: 25, completion tokens: 80
[2024-08-06 01:34:25.984] [info] llama_core in llama-core/src/lib.rs:433: Initializing the core context
[2024-08-06 01:34:54.760] [info] llama_core in llama-core/src/lib.rs:489: running mode: chat
[2024-08-06 01:34:54.760] [info] llama_core in llama-core/src/lib.rs:501: The core context has been initialized
[2024-08-06 01:34:54.760] [info] llama_core in llama-core/src/lib.rs:590: Getting the plugin info
[2024-08-06 01:34:54.760] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-06 01:34:54.761] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-06 01:34:54.761] [info] llama_core in llama-core/src/lib.rs:670: Getting the plugin info by the graph named default
[2024-08-06 01:34:54.761] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-06 01:34:54.761] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 95
[2024-08-06 01:34:54.762] [info] llama_core in llama-core/src/lib.rs:730: Plugin info: b2963(commit 95fb0aef)
[2024-08-07 13:27:26.061] [info] llama_core in llama-core/src/lib.rs:433: Initializing the core context
[2024-08-07 13:27:55.874] [info] llama_core in llama-core/src/lib.rs:489: running mode: chat
[2024-08-07 13:27:55.874] [info] llama_core in llama-core/src/lib.rs:501: The core context has been initialized
[2024-08-07 13:27:55.875] [info] llama_core in llama-core/src/lib.rs:590: Getting the plugin info
[2024-08-07 13:27:55.875] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-07 13:27:55.875] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-07 13:27:55.875] [info] llama_core in llama-core/src/lib.rs:670: Getting the plugin info by the graph named default
[2024-08-07 13:27:55.875] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-07 13:27:55.875] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 95
[2024-08-07 13:27:55.876] [info] llama_core in llama-core/src/lib.rs:730: Plugin info: b2963(commit 95fb0aef)
[2024-08-07 13:28:07.238] [info] llama_core::models in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/models.rs:9: List models
[2024-08-07 15:25:51.133] [info] llama_core in llama-core/src/lib.rs:433: Initializing the core context
[2024-08-07 15:25:55.060] [info] llama_core in llama-core/src/lib.rs:489: running mode: chat
[2024-08-07 15:25:55.060] [info] llama_core in llama-core/src/lib.rs:501: The core context has been initialized
[2024-08-07 15:25:55.061] [info] llama_core in llama-core/src/lib.rs:590: Getting the plugin info
[2024-08-07 15:25:55.061] [info] llama_core in llama-core/src/lib.rs:776: Get the running mode.
[2024-08-07 15:25:55.061] [info] llama_core in llama-core/src/lib.rs:801: running mode: chat
[2024-08-07 15:25:55.061] [info] llama_core in llama-core/src/lib.rs:670: Getting the plugin info by the graph named default
[2024-08-07 15:25:55.061] [info] llama_core::utils in llama-core/src/utils.rs:158: Get the output buffer generated by the model named default in the non-stream mode.
[2024-08-07 15:25:55.061] [info] llama_core::utils in llama-core/src/utils.rs:176: Output buffer size: 95
[2024-08-07 15:25:55.062] [info] llama_core in llama-core/src/lib.rs:730: Plugin info: b2963(commit 95fb0aef)
[2024-08-07 15:26:17.150] [info] llama_core::models in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/models.rs:9: List models
