[2024-08-06 00:40:43.662] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:633: Processing chat completion request in non-stream mode.
[2024-08-06 00:40:43.663] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:655: user: chatcmpl-3663403c-f3d8-46fc-b315-3004aa5f25b0
[2024-08-06 00:40:43.663] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1535: Check model metadata.
[2024-08-06 00:40:43.663] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 00:40:43.664] [info] llama_core::chat in llama-core/src/chat.rs:1824: Build the chat prompt from the chat messages.
[2024-08-06 00:40:43.664] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 00:40:43.664] [info] llama_core::chat in llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 00:40:47.134] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:666: prompt:
<|system|>
You are a helpful assistant.<|end|>
<|user|>
列出中 華民國歷任總統<|end|>
<|assistant|>
[2024-08-06 00:40:47.134] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:667: available_completion_tokens: 820
[2024-08-06 00:40:47.134] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:668: tool_use: false
[2024-08-06 00:40:47.134] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1641: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-08-06 00:40:47.134] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1651: update n_predict from 1024 to 820
[2024-08-06 00:40:47.134] [info] llama_core::chat in llama-core/src/chat.rs:2288: Update the model metadata.
[2024-08-06 00:40:47.135] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 00:40:48.282] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:692: Compute chat completion.
[2024-08-06 00:40:48.282] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:775: Compute chat completion by the model named default.
[2024-08-06 00:41:33.057] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:794: raw generation:  以下是中華民國總統的列表：

1. 李克勰

2. 韋賀道

3. 蔣介石

4. 蔡英文

5. 楊大山

6. 萬金

7. 宋純益

8. 郭英新

9. 鄧英

10. 李後慈

11. 趙耀業

12. 劉金星

13. 蔣介石

14. 韓天喬

15. 蘇翔

16. 廉恭行

17. 馬英鈞

18. 萍宏

19. 張大中

20. 陳大清

21. 陳超豪

22. 蔡慧民

23. 林前龍

24. 張政勳

25. 蘇翔

26. 馬英鈞

27. 陳明藝

28. 陳明中

29. 蔡英文

30. 劉連堅


這是一系列中國總統的清廖印表。<|end|>
[2024-08-06 00:41:33.057] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1684: Post-process the generated output.
[2024-08-06 00:41:33.057] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:802: post-processed generation:
以下是中華民國總統的列表：

1. 李克勰

2. 韋賀道

3. 蔣介石

4. 蔡英文

5. 楊大山

6. 萬金

7. 宋純益

8. 郭英新

9. 鄧英

10. 李後慈

11. 趙耀業

12. 劉金星

13. 蔣介石

14. 韓天喬

15. 蘇翔

16. 廉恭行

17. 馬英鈞

18. 萍宏

19. 張大中

20. 陳大清

21. 陳超豪

22. 蔡慧民

23. 林前龍

24. 張政勳

25. 蘇翔

26. 馬英鈞

27. 陳明藝

28. 陳明中

29. 蔡英文

30. 劉連堅


這是一系列中國總統的清廖印表。
[2024-08-06 00:41:33.060] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:808: prompt tokens: 29, completion tokens: 373
[2024-08-06 00:41:33.060] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:681: End of the chat completion.
[2024-08-06 00:41:55.312] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:633: Processing chat completion request in non-stream mode.
[2024-08-06 00:41:55.313] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:655: user: chatcmpl-a75be627-4ee0-4ab1-a36d-898551223dc0
[2024-08-06 00:41:55.313] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1535: Check model metadata.
[2024-08-06 00:41:55.313] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 00:41:55.314] [info] llama_core::chat in llama-core/src/chat.rs:1824: Build the chat prompt from the chat messages.
[2024-08-06 00:41:55.314] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 00:41:55.314] [info] llama_core::chat in llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 00:41:56.278] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:666: prompt:
<|system|>
You are a helpful assistant.<|end|>
<|user|>
What is the capital of Japan?<|end|>
<|assistant|>
[2024-08-06 00:41:56.278] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:667: available_completion_tokens: 820
[2024-08-06 00:41:56.278] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:668: tool_use: false
[2024-08-06 00:41:56.278] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1641: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-08-06 00:41:56.279] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1651: update n_predict from 1024 to 820
[2024-08-06 00:41:56.279] [info] llama_core::chat in llama-core/src/chat.rs:2288: Update the model metadata.
[2024-08-06 00:41:56.280] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 00:41:57.233] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:692: Compute chat completion.
[2024-08-06 00:41:57.234] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:775: Compute chat completion by the model named default.
[2024-08-06 00:42:02.094] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:794: raw generation:  The capital of Japan is Tokyo. It's one of the most populous metropolitan areas globally and serves as a center for government, business, media, culture, and transportation in Asia.<|end|>
[2024-08-06 00:42:02.094] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1684: Post-process the generated output.
[2024-08-06 00:42:02.094] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:802: post-processed generation:
The capital of Japan is Tokyo. It's one of the most populous metropolitan areas globally and serves as a center for government, business, media, culture, and transportation in Asia.
[2024-08-06 00:42:02.096] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:808: prompt tokens: 18, completion tokens: 42
[2024-08-06 00:42:02.096] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:681: End of the chat completion.
[2024-08-06 00:43:11.321] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:633: Processing chat completion request in non-stream mode.
[2024-08-06 00:43:11.322] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:655: user: chatcmpl-8b990ba6-b8c2-4464-814b-34478bb73bca
[2024-08-06 00:43:11.322] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1535: Check model metadata.
[2024-08-06 00:43:11.322] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 00:43:11.323] [info] llama_core::chat in llama-core/src/chat.rs:1824: Build the chat prompt from the chat messages.
[2024-08-06 00:43:11.323] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 00:43:11.323] [info] llama_core::chat in llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 00:43:12.773] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:666: prompt:
<|system|>
You are a helpful assistant.<|end|>
<|user|>
What is the capital of Japan?<|end|>
<|assistant|>
[2024-08-06 00:43:12.774] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:667: available_completion_tokens: 820
[2024-08-06 00:43:12.774] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:668: tool_use: false
[2024-08-06 00:43:12.774] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1641: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-08-06 00:43:12.774] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1651: update n_predict from 1024 to 820
[2024-08-06 00:43:12.774] [info] llama_core::chat in llama-core/src/chat.rs:2288: Update the model metadata.
[2024-08-06 00:43:12.775] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 00:43:13.537] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:692: Compute chat completion.
[2024-08-06 00:43:13.538] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:775: Compute chat completion by the model named default.
[2024-08-06 00:43:34.408] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:794: raw generation:  The capital of Japan is Tokyo. It serves as the political, economic, and cultural center of the country.

If you need more information or have any other questions, feel free to ask!<|end|>
[2024-08-06 00:43:34.408] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1684: Post-process the generated output.
[2024-08-06 00:43:34.409] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:802: post-processed generation:
The capital of Japan is Tokyo. It serves as the political, economic, and cultural center of the country.

If you need more information or have any other questions, feel free to ask!
[2024-08-06 00:43:34.411] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:808: prompt tokens: 19, completion tokens: 41
[2024-08-06 00:43:34.411] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:681: End of the chat completion.
[2024-08-06 00:43:39.746] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:633: Processing chat completion request in non-stream mode.
[2024-08-06 00:43:39.747] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:655: user: chatcmpl-9022e670-8137-4ff6-9a56-4ee21efbabac
[2024-08-06 00:43:39.747] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1535: Check model metadata.
[2024-08-06 00:43:39.747] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 00:43:39.747] [info] llama_core::chat in llama-core/src/chat.rs:1824: Build the chat prompt from the chat messages.
[2024-08-06 00:43:39.747] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 00:43:39.748] [info] llama_core::chat in llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 00:43:40.406] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:666: prompt:
<|system|>
You are a helpful assistant.<|end|>
<|user|>
What is the capital of Japan?<|end|>
<|assistant|>
[2024-08-06 00:43:40.406] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:667: available_completion_tokens: 820
[2024-08-06 00:43:40.407] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:668: tool_use: false
[2024-08-06 00:43:40.407] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1641: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-08-06 00:43:40.407] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1651: update n_predict from 1024 to 820
[2024-08-06 00:43:40.407] [info] llama_core::chat in llama-core/src/chat.rs:2288: Update the model metadata.
[2024-08-06 00:43:40.408] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 00:43:41.137] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:692: Compute chat completion.
[2024-08-06 00:43:41.137] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:775: Compute chat completion by the model named default.
[2024-08-06 00:43:43.723] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:794: raw generation:  The capital of Japan is Tokyo. It serves as the political, economic, and cultural center of the country.<|end|>
[2024-08-06 00:43:43.723] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1684: Post-process the generated output.
[2024-08-06 00:43:43.723] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:802: post-processed generation:
The capital of Japan is Tokyo. It serves as the political, economic, and cultural center of the country.
[2024-08-06 00:43:43.726] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:808: prompt tokens: 19, completion tokens: 23
[2024-08-06 00:43:43.726] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:681: End of the chat completion.
[2024-08-06 00:43:53.033] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:633: Processing chat completion request in non-stream mode.
[2024-08-06 00:43:53.033] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:655: user: chatcmpl-2edaecf6-bcc1-45c6-97c3-10b6e3edf6de
[2024-08-06 00:43:53.033] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1535: Check model metadata.
[2024-08-06 00:43:53.034] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 00:43:53.034] [info] llama_core::chat in llama-core/src/chat.rs:1824: Build the chat prompt from the chat messages.
[2024-08-06 00:43:53.034] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 00:43:53.034] [info] llama_core::chat in llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 00:43:53.678] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:666: prompt:
<|system|>
You are a helpful assistant.<|end|>
<|user|>
列出中 華民國歷任總統<|end|>
<|assistant|>
[2024-08-06 00:43:53.678] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:667: available_completion_tokens: 820
[2024-08-06 00:43:53.679] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:668: tool_use: false
[2024-08-06 00:43:53.679] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1641: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-08-06 00:43:53.679] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1651: update n_predict from 1024 to 820
[2024-08-06 00:43:53.679] [info] llama_core::chat in llama-core/src/chat.rs:2288: Update the model metadata.
[2024-08-06 00:43:53.680] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 00:43:54.368] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:692: Compute chat completion.
[2024-08-06 00:43:54.368] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:775: Compute chat completion by the model named default.
[2024-08-06 00:44:51.083] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:794: raw generation:  以字母順序，中國（台灣）的厝民國(臺灣)有六位總統如下：

1. 馬克思先生 (無法定於中華人民共和國政府)
2. 毛澤公（桅廉）
3. 宋月萱
4. 許世祺
5. 乞靡
6. 唐建國（張大鳴）
7. 李後漢
8. 郭景聲
9. 曾英信
10. 宋政英
11. 李大霸
12. 何倫道
13. 許優明
14. 陳新民
15. 謝中日
16. 王淳安
17. 周長清
18. 林毅邢
19. 郭元澤
20. 許允英
21. 曾志光
22. 李登光
23. 何以新
24. 周文達
25. 鄧中仁
26. 謝雅緯
27. 陳百友
28. 吳冠義
29. 王廉忠
30. 曾小平
31. 孫祖章
32. 郑先生
33. 林夕賢
34. 桅宣勲
35. 毛澤公（愈）
36. 陳百翎
37. 李克定
38. 許志雄
39. 孫祖章（二代）
40. 陳百翎（二代）
41. 桅宣勲（二代）
42. 鄧先生（第三代）
43. 陳英明
44. 林志全
45. 許民信
46. 謝中緯（二代）
47. 許允義
48. 孫濟道
49. 李政德
50. 許世賢
51. 郭元延
52. 王宏達
53. 陳日華
54. 曾国陵
55. 秦濤易
56. 李誠聰
57. 王承鋄
58. 吳逸達
59. 陳政道
60. 林文青

(Note: 馬克思先生不是中华人民共和国的總統，他被認為是世界史上最受歡迎的革命家和思想家之一。)<|end|>
[2024-08-06 00:44:51.083] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1684: Post-process the generated output.
[2024-08-06 00:44:51.084] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:802: post-processed generation:
以字母順序，中國（台灣）的厝民國(臺灣)有六位總統如下：

1. 馬克思先生 (無法定於中華人民共和國政府)
2. 毛澤公（桅廉）
3. 宋月萱
4. 許世祺
5. 乞靡
6. 唐建國（張大鳴）
7. 李後漢
8. 郭景聲
9. 曾英信
10. 宋政英
11. 李大霸
12. 何倫道
13. 許優明
14. 陳新民
15. 謝中日
16. 王淳安
17. 周長清
18. 林毅邢
19. 郭元澤
20. 許允英
21. 曾志光
22. 李登光
23. 何以新
24. 周文達
25. 鄧中仁
26. 謝雅緯
27. 陳百友
28. 吳冠義
29. 王廉忠
30. 曾小平
31. 孫祖章
32. 郑先生
33. 林夕賢
34. 桅宣勲
35. 毛澤公（愈）
36. 陳百翎
37. 李克定
38. 許志雄
39. 孫祖章（二代）
40. 陳百翎（二代）
41. 桅宣勲（二代）
42. 鄧先生（第三代）
43. 陳英明
44. 林志全
45. 許民信
46. 謝中緯（二代）
47. 許允義
48. 孫濟道
49. 李政德
50. 許世賢
51. 郭元延
52. 王宏達
53. 陳日華
54. 曾国陵
55. 秦濤易
56. 李誠聰
57. 王承鋄
58. 吳逸達
59. 陳政道
60. 林文青

(Note: 馬克思先生不是中华人民共和国的總統，他被認為是世界史上最受歡迎的革命家和思想家之一。)
[2024-08-06 00:44:51.086] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:808: prompt tokens: 30, completion tokens: 788
[2024-08-06 00:44:51.086] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:681: End of the chat completion.
[2024-08-06 01:15:57.967] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:633: Processing chat completion request in non-stream mode.
[2024-08-06 01:15:57.968] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:655: user: chatcmpl-11676a1b-20fb-4635-82e3-94148d828285
[2024-08-06 01:15:57.968] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1535: Check model metadata.
[2024-08-06 01:15:57.968] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 01:15:57.969] [info] llama_core::chat in llama-core/src/chat.rs:1824: Build the chat prompt from the chat messages.
[2024-08-06 01:15:57.969] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 01:15:57.969] [info] llama_core::chat in llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 01:15:59.723] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:666: prompt:
<|system|>
You are a helpful assistant.<|end|>
<|user|>
列出中華民國歷任總統<|end|>
<|assistant|>
[2024-08-06 01:15:59.723] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:667: available_completion_tokens: 820
[2024-08-06 01:15:59.723] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:668: tool_use: false
[2024-08-06 01:15:59.723] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1641: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-08-06 01:15:59.723] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1651: update n_predict from 1024 to 820
[2024-08-06 01:15:59.724] [info] llama_core::chat in llama-core/src/chat.rs:2288: Update the model metadata.
[2024-08-06 01:15:59.724] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 01:16:00.409] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:692: Compute chat completion.
[2024-08-06 01:16:00.409] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:775: Compute chat completion by the model named default.
[2024-08-06 01:16:43.176] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:794: raw generation:  中華民國（民主 República de China）的歷任總統如下：

1. 許冠裕 (Xuân Quang Hùng) - 2016年7月4日至2017年3月25日

2. 陳以仁 (Trần Ít Toàn) - 2017年3月25日至令人舊錢2021年2月7日

3. 許安基 (Xǔ Ānjī) - 2021年2月7日至撥破時間2028年5月10日（測算這部分，因為他的任期未到底）

4. 唐信 (Tang Xin) - 2028年5月10日至

(注意：中華民國是基于台湾來自的政府總统治，但對於世界層次上其他中華民國所創建的歷史觀察中，我們以台北總督府的任期作為中華民國的總統任期。此外，這部分信息仍在2021年時間沒有收到更新。)<|end|>
[2024-08-06 01:16:43.176] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1684: Post-process the generated output.
[2024-08-06 01:16:43.176] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:802: post-processed generation:
中華民國（民主 República de China）的歷任總統如下：

1. 許冠裕 (Xuân Quang Hùng) - 2016年7月4日至2017年3月25日

2. 陳以仁 (Trần Ít Toàn) - 2017年3月25日至令人舊錢2021年2月7日

3. 許安基 (Xǔ Ānjī) - 2021年2月7日至撥破時間2028年5月10日（測算這部分，因為他的任期未到底）

4. 唐信 (Tang Xin) - 2028年5月10日至

(注意：中華民國是基于台湾來自的政府總统治，但對於世界層次上其他中華民國所創建的歷史觀察中，我們以台北總督府的任期作為中華民國的總統任期。此外，這部分信息仍在2021年時間沒有收到更新。)
[2024-08-06 01:16:43.178] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:808: prompt tokens: 29, completion tokens: 364
[2024-08-06 01:16:43.178] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:681: End of the chat completion.
[2024-08-06 01:17:14.783] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:633: Processing chat completion request in non-stream mode.
[2024-08-06 01:17:14.784] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:655: user: chatcmpl-a4daf967-3686-4ac0-b72c-ff2260c3eca1
[2024-08-06 01:17:14.784] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1535: Check model metadata.
[2024-08-06 01:17:14.784] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 01:17:14.784] [info] llama_core::chat in llama-core/src/chat.rs:1824: Build the chat prompt from the chat messages.
[2024-08-06 01:17:14.785] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 01:17:14.785] [info] llama_core::chat in llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 01:17:15.401] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:666: prompt:
<|system|>
You are a helpful assistant.<|end|>
<|user|>
What is the capital of Taiwan, Japan, USA, and China?<|end|>
<|assistant|>
[2024-08-06 01:17:15.401] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:667: available_completion_tokens: 820
[2024-08-06 01:17:15.402] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:668: tool_use: false
[2024-08-06 01:17:15.402] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1641: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-08-06 01:17:15.402] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1651: update n_predict from 1024 to 820
[2024-08-06 01:17:15.402] [info] llama_core::chat in llama-core/src/chat.rs:2288: Update the model metadata.
[2024-08-06 01:17:15.403] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 01:17:16.065] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:692: Compute chat completion.
[2024-08-06 01:17:16.065] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:775: Compute chat completion by the model named default.
[2024-08-06 01:17:26.728] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:794: raw generation:  The capital cities of these countries are as follows:

1. Taiwan: Taipei
2. Japan: Tokyo
3. USA: Washington, D.C.
4. China: Beijing

Please note that while the People's Republic of China considers itself to be the sole legal government representing "China," there is an additional entity called the Republic of China (Taiwan) which has its own capital, Taipei.

Also, it's essential to mention that Taiwan and the area governed by the Chinese authorities both refer to themselves as 'China', but they are distinct political entities with different administrative structures.<|end|>
[2024-08-06 01:17:26.728] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1684: Post-process the generated output.
[2024-08-06 01:17:26.728] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:802: post-processed generation:
The capital cities of these countries are as follows:

1. Taiwan: Taipei
2. Japan: Tokyo
3. USA: Washington, D.C.
4. China: Beijing

Please note that while the People's Republic of China considers itself to be the sole legal government representing "China," there is an additional entity called the Republic of China (Taiwan) which has its own capital, Taipei.

Also, it's essential to mention that Taiwan and the area governed by the Chinese authorities both refer to themselves as 'China', but they are distinct political entities with different administrative structures.
[2024-08-06 01:17:26.730] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:808: prompt tokens: 26, completion tokens: 138
[2024-08-06 01:17:26.730] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:681: End of the chat completion.
[2024-08-06 01:22:53.886] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:633: Processing chat completion request in non-stream mode.
[2024-08-06 01:22:53.888] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:655: user: chatcmpl-b84096ab-7103-4f4a-90fd-e44979d55b1c
[2024-08-06 01:22:53.888] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1535: Check model metadata.
[2024-08-06 01:22:53.888] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 01:22:53.888] [info] llama_core::chat in llama-core/src/chat.rs:1824: Build the chat prompt from the chat messages.
[2024-08-06 01:22:53.888] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 01:22:53.889] [info] llama_core::chat in llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 01:22:55.770] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:666: prompt:
<|system|>
You are a helpful assistant.<|end|>
<|user|>
What is the capital of Taiwan, Japan, USA, and China?<|end|>
<|assistant|>
[2024-08-06 01:22:55.770] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:667: available_completion_tokens: 820
[2024-08-06 01:22:55.771] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:668: tool_use: false
[2024-08-06 01:22:55.771] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1641: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-08-06 01:22:55.771] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1651: update n_predict from 1024 to 820
[2024-08-06 01:22:55.771] [info] llama_core::chat in llama-core/src/chat.rs:2288: Update the model metadata.
[2024-08-06 01:22:55.772] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 01:22:56.537] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:692: Compute chat completion.
[2024-08-06 01:22:56.537] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:775: Compute chat completion by the model named default.
[2024-08-06 01:23:26.541] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:794: raw generation:  The capital of Taiwan is Taipei.
The capital city in Japan isn't unique—there are 47 prefectures and Tokyo, which serves as the political center where most government offices reside; hence it’s considered a de facto capital for many purposes despite not having an official title such as "capital".

USA doesn't have just one national capital. Washington D.C., officially known as District of Columbia (District), is recognized and functions in this capacity due to the U.S. Constitution, which established it specifically; however, all states also possess their own state capitals such as Albany for New York or Austin for Texas where each operates its individual governmental offices separate from Washington D.C.'s federal-level roles.

China's capital is Beijing, one of the world’s most populous cities and a place rich in culture, history & politics.<|end|>
[2024-08-06 01:23:26.541] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1684: Post-process the generated output.
[2024-08-06 01:23:26.542] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:802: post-processed generation:
The capital of Taiwan is Taipei.
The capital city in Japan isn't unique—there are 47 prefectures and Tokyo, which serves as the political center where most government offices reside; hence it’s considered a de facto capital for many purposes despite not having an official title such as "capital".

USA doesn't have just one national capital. Washington D.C., officially known as District of Columbia (District), is recognized and functions in this capacity due to the U.S. Constitution, which established it specifically; however, all states also possess their own state capitals such as Albany for New York or Austin for Texas where each operates its individual governmental offices separate from Washington D.C.'s federal-level roles.

China's capital is Beijing, one of the world’s most populous cities and a place rich in culture, history & politics.
[2024-08-06 01:23:26.543] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:808: prompt tokens: 25, completion tokens: 195
[2024-08-06 01:23:26.544] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:681: End of the chat completion.
[2024-08-06 01:29:12.783] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:633: Processing chat completion request in non-stream mode.
[2024-08-06 01:29:12.785] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:655: user: chatcmpl-d0905e2c-ee3f-4487-8890-d2d2087f2c94
[2024-08-06 01:29:12.785] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1535: Check model metadata.
[2024-08-06 01:29:12.785] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 01:29:12.785] [info] llama_core::chat in llama-core/src/chat.rs:1824: Build the chat prompt from the chat messages.
[2024-08-06 01:29:12.785] [info] llama_core::chat in llama-core/src/chat.rs:2206: Get the model metadata.
[2024-08-06 01:29:12.786] [info] llama_core::chat in llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 01:29:14.454] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:666: prompt:
<|system|>
You are a helpful assistant.<|end|>
<|user|>
What is the capital of Taiwan, Japan, USA, and China?<|end|>
<|assistant|>
[2024-08-06 01:29:14.454] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:667: available_completion_tokens: 820
[2024-08-06 01:29:14.454] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:668: tool_use: false
[2024-08-06 01:29:14.455] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1641: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-08-06 01:29:14.455] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1651: update n_predict from 1024 to 820
[2024-08-06 01:29:14.455] [info] llama_core::chat in llama-core/src/chat.rs:2288: Update the model metadata.
[2024-08-06 01:29:14.456] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:2102: Set prompt to the chat model named default.
[2024-08-06 01:29:15.165] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:692: Compute chat completion.
[2024-08-06 01:29:15.165] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:775: Compute chat completion by the model named default.
[2024-08-06 01:29:39.555] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:794: raw generation:  The capital of Taiwan is Taipei, the capital of Japan is Tokyo, the United States's capital city includes Washington D.C., which houses federal government offices and foreign embassies (although technically a special district), while China considers Beijing as its political center even though it hosts most central institutions in Xicheng District where Tiananmen Square resides.<|end|>
[2024-08-06 01:29:39.555] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:1684: Post-process the generated output.
[2024-08-06 01:29:39.555] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:802: post-processed generation:
The capital of Taiwan is Taipei, the capital of Japan is Tokyo, the United States's capital city includes Washington D.C., which houses federal government offices and foreign embassies (although technically a special district), while China considers Beijing as its political center even though it hosts most central institutions in Xicheng District where Tiananmen Square resides.
[2024-08-06 01:29:39.557] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:808: prompt tokens: 25, completion tokens: 80
[2024-08-06 01:29:39.557] [info] llama_core::chat in /home/runner/work/LlamaEdge/LlamaEdge/api-server/llama-core/src/chat.rs:681: End of the chat completion.
